\documentclass[name={Jianxun Zhou}, andrewid={zhou-jx23}, course={cs285}, num={2}]{homework}

\usepackage{hw-shortcuts}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}

\begin{document}
\experiment{1}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{./images/exp1_cartpole.png}
    \caption{\centering \small \textbf{Eval Return} vs. \textbf{Env Steps}}
    \label{fig:Eval_Return_Compare}
\end{figure}
1. Without \textbf{advantage normalization}, the \textbf{reward-to-go} 
   value estimator performs better than the \textbf{trajectory-centric} one.

2. \textbf{advantage normalization} helps the policy to achieve the same 
   performance in \textbf{fewer environment step}.

3. In fact, the training with a \textbf{larger batch size} achieves the same 
    performance in \textbf{more environment steps} but \textbf{fewer training 
    iteration steps}.
    
4. The command line configuration i use is totally the same as the one given 
   in homework document.

\newpage

\experiment{2}
1. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{./images/exp2_1_baseline_blr01.png}
    \caption{\centering \small \textbf{Baseline Loss} vs. \textbf{Env Steps} with baseline learning rate=0.01}
    \label{fig: Baseline_blr0.01}
\end{figure}
2. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{./images/exp2_3_return_vs_vanilla.png}
    \caption{\centering \small \textbf{Average Return} vs. \textbf{Env Steps} with baseline learning rate=0.01}
    \label{fig:Baseline_Return_Compare}
\end{figure}
3. 

\begin{figure}[H]
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{./images/exp2_2_baseline_compare.png}
        \caption{\centering \small Baseline Loss with different baseline learning rate}
        \label{fig:baseline_value_compare}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{./images/exp2_4_return_two_blr.png}
        \caption{\centering \small Baseline Return with different baseline learning rate}
        \label{fig:baseline_return_compare}
    \end{subfigure}
Compared to \textbf{baseline learning rate = 0.01}, training with \textbf{baseline learning rate 0.001}
causes slower \textbf{baseline loss convergence} and a slower increase in \textbf{average return}.
\end{figure}

\experiment{3}
1. 
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.88\textwidth]{./images/exp3_return.png}
        \caption{\centering \small \textbf{Average Return} vs. \textbf{Env Steps} with different GAE lambda}
        \label{fig:GAE_lambda}
    \end{figure}
    While increasing $\lambda$ generally improves policy performance, an excessively large $\lambda$ leads
    to instabilities in the later stages of training.

2.  $\lambda = 0$ correspond to $\hat{A}(s_t, a_t) = r(s_t, a_t) + \gamma \hat{V}(s_{t+1}) - \hat{A}(s_t)$, 
    which means we totally use prediction from critic as the estimation of state value. At the beginning of 
    training, the critic's predictions are \textbf{highly biased}, which leads to \textbf{suboptimal} policy 
    performance. However, because the critic's predictions have \textbf{low variance}, the resulting training 
    curves exhibit fewer oscillations.

    $\lambda = 1$ correspond to $\hat{A}(s_t, a_t) = \sum_{t' = t}^{T}\gamma^{t'-t}r(s_{t'}, a_{t'}) - 
    \hat{V}(s_t)$, which means we totally use the following steps' rewards from environment to estimate
    the advantage of $a_t$. This causes high variance since the estimate is based on the sum of actual 
    rewards, it is unbiased but suffers from high variance due to the stochasticity of the environment.

    In the figure \ref{fig:GAE_lambda}, the blue curve($\lambda=0$)'s $eval\_return$ struggles near $-100$ 
    because of huge bias caused by estimating the value totally depend on critic. And other four curves 
    obviously perform better than the blue one. However, due to the high variance from stochastic step reward
    from environment, sometimes the return falls down violently.
    
\experiment{4}
1. \textbf{Hyperparameters}
\begin{table}[H]
    \centering
    \begin{tabularx}{0.5\textwidth}{lr}
        \toprule
        \textbf{Parameter}&\textbf{Value} \\
        \midrule
        iteration&100 \\
        batch size&500 \\
        \midrule
        learning rate&0.01 \\
        discount&0.99 \\
        GAE lambda&0.98 \\
        reward to go&true \\
        \midrule
        use baseline&true \\
        baseline learning rate&0.01 \\
        baseline gradient steps&5 \\
        \midrule
        n layers&2 \\
        layer size&64 \\
        \bottomrule
    \end{tabularx}
    \caption{Hyperparameters}
\end{table}
2. \textbf{Return Curves}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\textwidth]{./images/comparison_plot.png}
    \caption{Eval Average Return-Env Steps(default VS. self-define)}
    \label{fig:hyperparam_tuning}
\end{figure}

\section{Analysis}
\subsection{Apply policy gradients}
a. 
\begin{align*}
\nabla _{\theta} J(\theta) &= \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[\nabla_{\theta}\log P_\theta(\tau) r(\tau)] \\
&= \sum_{k=1}^{\infty}\theta^{k-1}(1-\theta) \cdot \left(\frac{k}{\theta} - \frac{1}{1-\theta}\right) \cdot k \\
&= (1 - \theta) \sum_{k=1}^{\infty} k^2\theta^{k-1} - \theta\sum_{k=1}^{\infty}k\theta^{k-1} \\
&= (1 - \theta) \cdot \frac{1+\theta}{(1-\theta)^{3}} - \theta \cdot \frac{1}{(1-\theta)^2} \\
&= \frac{1}{(1-\theta)^2}
\end{align*}
b. Calculate $J(\theta) = \mathbb{E}_{\pi_{\theta}}[R(\tau)]$ recursively.
\begin{align*}
J(\theta) = \theta \cdot (1 + J(\theta)) + (1 - \theta) \cdot 0 \\
\Rightarrow J(\theta) = \frac{\theta}{1 - \theta}
\end{align*}
Differentiate with respect to $\theta$:
$$
\nabla_\theta J(\theta) = \frac{1}{(1-\theta)^2}
$$
This matches the result from the direct calculation.

\subsection{Policy gradient variance}
\begin{align*}
    \mathbb{E}[(\nabla_\theta J(\theta))^2] &= \sum _{k=1}^{\infty}\theta^{k-1}(1-\theta) \cdot \left[\left(\frac{k}{\theta} - \frac{1}{1-\theta}\right)k\right]^2 \\
    &= \sum_{k=1}^{\infty} \left(\frac{k^4}{\theta^2} + \frac{k^2}{(1-\theta)^2} - \frac{2k^3}{\theta(1-\theta)}\right) \theta^{k-1}(1-\theta) \\
    &= \frac{1-\theta}{\theta^2}\sum_{k=1}^{\infty}k^4\theta^{k-1} + \frac{1}{1-\theta}\sum_{k=1}^{\infty}k^2\theta^{k-1} - \frac{2}{\theta} \sum_{k=1}^{\infty}k^3\theta^{k-1} \\
    &= \dots \\
    &= \frac{1 + 9\theta + 4 \theta^2}{\theta(1-\theta)^4}
\end{align*}
Note: The intermediate steps involve calculating sums of the form $\sum k^n x^k$. After simplification (omitted for brevity), we arrive at the variance term.

From equation $Var(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$:
\begin{align*}
    Var[\nabla_{\theta}J(\theta)] &= \mathbb{E}[(\nabla_{\theta}J(\theta))^2] - (\mathbb{E}[\nabla_{\theta}J(\theta)])^2 \\
    &= \frac{1 + 9\theta + 4 \theta^2}{\theta(1-\theta)^4} - \left(\frac{1}{(1-\theta)^2}\right)^2 \\
    &= \frac{1 + 9\theta + 4 \theta^2 -\theta}{\theta(1-\theta)^4}\\
    &= \frac{4\theta^2+8\theta+1}{\theta(1-\theta)^4}
\end{align*}

\subsection{Apply reward-to-go}
a. 
For a trajectory with horizon $k$, the reward-to-go of step $t$ is $(k-t)$ if $t\leq k$ and $0$ if $t = k$.
Let $G_{\tau_k}$ denote the policy gradient of trajectory with horizon $k$.
\begin{align*}
    G_{\tau_k} &= \sum_{t=0}^{k-1}\frac{1}{\theta}(k-t) + 0 \\
    &= \sum_{t=0}^{k-1} \frac{k}{\theta} - \sum_{t=0}^{k-1}\frac{t}{\theta} \\
    &= \frac{k(k-1)}{2\theta}
\end{align*}
Expectation of $G_{\tau_k}$:
\begin{align*}
    \nabla_{\theta}J(\theta) &= \sum_{k=1}^{\infty}G_{\tau_k} \cdot \theta^{k-1}(1-\theta) \\
    &= \frac{1-\theta}{2} \sum_{k=1}^{\infty}k(k-1)\theta^{k-2} \\
    &= \frac{1-\theta}{2}\frac{d^2}{d\theta^2} \sum_{k=1}^{\infty}\theta^k \\
    &= \frac{1-\theta}{2}\frac{d^2}{d\theta^2}\frac{\theta}{1-\theta} \\
    &= \frac{1-\theta}{2} \frac{2(1-\theta)}{(1-\theta)^4} \\
    &= \frac{1}{(1-\theta)^2}
\end{align*}
This form matches the one without reward-to-go, confirming that reward-to-go estimator is unbiased.

b. 
With reward-to-go:
\begin{align*}
    \mathbb{E}[(\nabla_{\theta}J(\theta))^2] &= \sum_{k=1}^{\infty}\theta^{k-1}(1-\theta)\cdot \frac{k^2(k-1)^2}{4\theta^2} \\
    &= \dots \\
    &= \frac{1 + 4\theta + \theta^2}{\theta(1-\theta)^4}
\end{align*}
Variance:
\begin{align*}
    Var(\nabla_{\theta}J(\theta)) &= \frac{1 + 4\theta + \theta^2}{\theta(1-\theta)^4} - (\frac{1}{(1-\theta)^2})^2 \\
    &= \frac{\theta^2 + 3\theta + 1}{\theta(1-\theta)^4}
\end{align*}
\begin{figure}
    \centering
    \includegraphics[width=0.98\textwidth]{./images/var_compare.png}
    \caption{\textbf{Variance Comparison(reward-to-go VS. default)}}
    \label{fig:var_cmp}
\end{figure}
\subsection{Importance Sampling}
a. 
Only the trajectory ending at $s_{H}$ contributes to the expectation of Policy Gradient.
\begin{align*}
    J(\theta) &= \mathbb{E}_{\tau \sim p_{\theta '(\tau)}}[\frac{p_\theta(\tau)}{p_{\theta '}(\tau)} r(\tau)] \\
    &= \theta '^{H-1} \cdot (\frac{\theta}{\theta '})^{H-1} \cdot 1 \\
    &= \theta^{H-1}
\end{align*}
Differentiate with respect to $\theta$:
$$
    \nabla_{\theta}J(\theta) = (H-1)\theta^{H-2}
$$

b. 
\begin{align*}
    \mathbb{E}[(\nabla_\theta J(\theta))^2] &= \theta '^{H-1}\cdot [(\frac{\theta}{\theta '})^{H-1} \cdot \frac{H-1}{\theta} \cdot 1]^2 \\
    &= \frac{(H-1)^2 \cdot \theta^{2H-4}}{\theta '^{H-1}}
\end{align*}
Variance:
\begin{align*}
    Var[\nabla_\theta J(\theta)] &= \mathbb{E}[(\nabla_\theta J(\theta))^2] - \mathbb{E}^2[(\nabla_\theta J(\theta))] \\
    &= (H-1)^2\theta^{2H-4}(\frac{1}{\theta '^{H-1}} - 1)
\end{align*}

As $H$ increases, the variance grows exponentially.
\end{document}